[
  {
    "topic": "MCP Overview and Architecture",
    "type": "fundamental",
    "difficulty": "beginner",
    "content": "Model Context Protocol (MCP) is a standardized protocol that enables Large Language Models (LLMs) to interact with external tools, data sources, and services in a vendor-neutral way. Unlike proprietary function calling APIs, MCP provides a universal interface that works across different LLM providers.\n\nKey Components:\n1. MCP Server: Exposes tools and resources to clients\n2. MCP Client: Consumes tools (typically integrated with LLM applications)\n3. Transport Layer: Handles communication (stdio, HTTP, WebSocket)\n4. Protocol Layer: Defines message formats and schemas\n\nArchitecture Benefits:\n- Protocol Independence: Not tied to specific LLM providers\n- Bidirectional Communication: Supports both tool calls and resource access\n- State Management: Can maintain context across interactions\n- Standardized Schemas: Uses JSON Schema for consistent tool definitions"
  },
  {
    "topic": "MCP vs OpenAI Function Calling",
    "type": "comparison",
    "difficulty": "beginner",
    "content": "While both MCP and OpenAI Function Calling enable LLMs to use external tools, they have key differences:\n\nMCP Advantages:\n✓ Vendor-neutral protocol\n✓ Works with any LLM (Claude, Gemini, local models)\n✓ Supports persistent connections and state\n✓ Bidirectional communication\n✓ Resource management capabilities\n✓ Transport flexibility (stdio, HTTP, WebSocket)\n\nOpenAI Function Calling:\n✓ Native integration with OpenAI models\n✓ Simpler setup for OpenAI-only use cases\n✓ Built-in to ChatGPT API\n\nSchema Similarities:\n- Both use JSON Schema for tool definitions\n- Similar parameter validation approaches\n- Function description patterns are comparable\n\nChoose MCP when:\n- Using multiple LLM providers\n- Need persistent state/context\n- Want vendor independence\n- Building complex tool ecosystems"
  },
  {
    "topic": "Basic MCP Server Implementation",
    "type": "implementation",
    "difficulty": "beginner",
    "content": "Creating a basic MCP server involves extending the Server class and registering tools:\n\n```python\nfrom mcp.server import Server\nimport asyncio\n\nserver = Server(\"my-mcp-server\")\n\n@server.tool()\nasync def calculate_sum(a: int, b: int) -> int:\n    \"\"\"Calculate the sum of two numbers\"\"\"\n    return a + b\n\n@server.tool()\nasync def greet_user(name: str, greeting: str = \"Hello\") -> str:\n    \"\"\"Greet a user with a custom message\"\"\"\n    return f\"{greeting}, {name}!\"\n\n@server.tool()\nasync def get_system_info() -> dict:\n    \"\"\"Get basic system information\"\"\"\n    import platform\n    return {\n        \"system\": platform.system(),\n        \"python_version\": platform.python_version(),\n        \"machine\": platform.machine()\n    }\n\nif __name__ == \"__main__\":\n    asyncio.run(server.run())\n```\n\nKey Points:\n- Use @server.tool() decorator for automatic registration\n- Function docstrings become tool descriptions\n- Type hints define parameter schemas\n- Async functions are required for proper MCP operation"
  },
  {
    "topic": "Tool Schema Definition and Validation",
    "type": "implementation",
    "difficulty": "intermediate",
    "content": "Proper tool schemas ensure reliable parameter validation and clear documentation:\n\n```python\nfrom mcp.types import Tool\nfrom typing import Optional, Literal\n\n# Method 1: Using type hints (recommended)\n@server.tool()\nasync def search_files(\n    query: str,\n    file_type: Literal[\"text\", \"image\", \"video\"] = \"text\",\n    max_results: int = 10,\n    include_hidden: bool = False\n) -> list[dict]:\n    \"\"\"Search for files matching criteria\n    \n    Args:\n        query: Search term or pattern\n        file_type: Type of files to search for\n        max_results: Maximum number of results to return\n        include_hidden: Whether to include hidden files\n    \"\"\"\n    # Implementation here\n    pass\n\n# Method 2: Explicit schema definition\nweather_tool = Tool(\n    name=\"get_weather\",\n    description=\"Get current weather information for a location\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"city\": {\n                \"type\": \"string\",\n                \"description\": \"The city name\"\n            },\n            \"units\": {\n                \"type\": \"string\",\n                \"enum\": [\"celsius\", \"fahrenheit\", \"kelvin\"],\n                \"default\": \"celsius\",\n                \"description\": \"Temperature units\"\n            },\n            \"include_forecast\": {\n                \"type\": \"boolean\",\n                \"default\": false,\n                \"description\": \"Include 5-day forecast\"\n            }\n        },\n        \"required\": [\"city\"]\n    }\n)\n```\n\nBest Practices:\n- Use descriptive parameter names\n- Provide clear descriptions\n- Set appropriate defaults\n- Use enums for limited choices\n- Mark required vs optional parameters"
  },
  {
    "topic": "Error Handling and Debugging",
    "type": "troubleshooting",
    "difficulty": "intermediate",
    "content": "Common MCP issues and debugging strategies:\n\n**Issue 1: Tool returns null/empty**\nCauses:\n- Missing return statement\n- Incorrect async/await usage\n- Type annotation mismatch\n- Serialization problems\n\nSolution:\n```python\n@server.tool()\nasync def debug_tool(param: str) -> str:\n    logger.debug(f\"Received: {param}\")\n    try:\n        result = await process_data(param)\n        logger.debug(f\"Returning: {result}\")\n        return result\n    except Exception as e:\n        logger.error(f\"Error: {e}\")\n        raise\n```\n\n**Issue 2: Schema validation errors**\n```python\n# Bad: Inconsistent types\n@server.tool()\nasync def bad_tool(count: int) -> str:\n    return count  # Returns int, not str!\n\n# Good: Consistent types\n@server.tool()\nasync def good_tool(count: int) -> str:\n    return str(count)\n```\n\n**Issue 3: Connection problems**\n- Check transport configuration\n- Verify server startup\n- Test client connection settings\n- Use debug logging\n\n**Debugging Setup:**\n```python\nimport logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n```"
  },
  {
    "topic": "State Management and Context",
    "type": "advanced",
    "difficulty": "advanced",
    "content": "Managing state and context across tool calls:\n\n```python\nclass StatefulMCPServer:\n    def __init__(self):\n        self.server = Server(\"stateful-server\")\n        self.session_data = {}\n        self.global_config = {}\n        self._register_tools()\n    \n    def _register_tools(self):\n        @self.server.tool()\n        async def set_user_preference(key: str, value: str) -> str:\n            \"\"\"Set a user preference for the session\"\"\"\n            self.session_data[key] = value\n            return f\"Set {key} = {value}\"\n        \n        @self.server.tool()\n        async def get_user_preference(key: str) -> str:\n            \"\"\"Get a user preference from the session\"\"\"\n            return self.session_data.get(key, \"Not set\")\n        \n        @self.server.tool()\n        async def process_with_context(data: str) -> dict:\n            \"\"\"Process data using stored context\"\"\"\n            user_lang = self.session_data.get(\"language\", \"en\")\n            user_format = self.session_data.get(\"output_format\", \"json\")\n            \n            # Process data with context\n            result = {\n                \"processed_data\": data,\n                \"language\": user_lang,\n                \"format\": user_format,\n                \"timestamp\": datetime.now().isoformat()\n            }\n            \n            return result\n        \n        @self.server.tool()\n        async def clear_session() -> str:\n            \"\"\"Clear all session data\"\"\"\n            self.session_data.clear()\n            return \"Session cleared\"\n\n# Usage pattern for workflows\nserver = StatefulMCPServer()\n```\n\n**Context Patterns:**\n1. Session State: User preferences, temporary data\n2. Global State: Configuration, shared resources\n3. Workflow State: Multi-step process tracking\n4. Cache Management: Performance optimization"
  },
  {
    "topic": "Transport Protocols and Connection Management",
    "type": "implementation",
    "difficulty": "intermediate",
    "content": "MCP supports multiple transport protocols for different use cases:\n\n**1. Standard I/O (stdio) - Most Common**\n```python\n# Server side\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(server.run())\n\n# Client connection\nfrom mcp.client import Client\nclient = Client(transport=\"stdio\", command=[\"python\", \"my_server.py\"])\n```\n\n**2. HTTP Transport**\n```python\n# Server with HTTP\nfrom mcp.server.http import HTTPServer\n\nhttp_server = HTTPServer(server, host=\"localhost\", port=8080)\nasyncio.run(http_server.start())\n\n# Client connection\nclient = Client(transport=\"http\", url=\"http://localhost:8080\")\n```\n\n**3. WebSocket Transport**\n```python\n# Server with WebSocket\nfrom mcp.server.websocket import WebSocketServer\n\nws_server = WebSocketServer(server, host=\"localhost\", port=8081)\nasyncio.run(ws_server.start())\n\n# Client connection\nclient = Client(transport=\"websocket\", url=\"ws://localhost:8081\")\n```\n\n**Transport Selection Guidelines:**\n- stdio: CLI tools, simple integrations\n- HTTP: Web services, REST-like interactions\n- WebSocket: Real-time, persistent connections\n\n**Connection Lifecycle:**\n```python\nasync def manage_connection():\n    async with client:\n        tools = await client.list_tools()\n        result = await client.call_tool(\"my_tool\", {\"param\": \"value\"})\n        return result\n```"
  },
  {
    "topic": "Resource Management in MCP",
    "type": "advanced",
    "difficulty": "advanced",
    "content": "MCP supports resource management for handling files, databases, and other persistent data:\n\n```python\nfrom mcp.server import Server\nfrom mcp.types import Resource, ResourceTemplate\n\nserver = Server(\"resource-server\")\n\n# Define resource templates\nfile_template = ResourceTemplate(\n    uriTemplate=\"file://{path}\",\n    name=\"Local File\",\n    description=\"Access to local files\",\n    mimeType=\"text/plain\"\n)\n\ndb_template = ResourceTemplate(\n    uriTemplate=\"db://{table}/{id}\",\n    name=\"Database Record\",\n    description=\"Access to database records\",\n    mimeType=\"application/json\"\n)\n\n@server.resource_template(file_template)\nasync def get_file_resource(path: str) -> Resource:\n    \"\"\"Get file content as a resource\"\"\"\n    try:\n        with open(path, 'r') as f:\n            content = f.read()\n        \n        return Resource(\n            uri=f\"file://{path}\",\n            name=f\"File: {path}\",\n            description=f\"Contents of {path}\",\n            mimeType=\"text/plain\",\n            text=content\n        )\n    except FileNotFoundError:\n        raise ValueError(f\"File not found: {path}\")\n\n@server.resource_template(db_template)\nasync def get_db_resource(table: str, id: str) -> Resource:\n    \"\"\"Get database record as a resource\"\"\"\n    # Mock database access\n    record = await fetch_from_db(table, id)\n    \n    return Resource(\n        uri=f\"db://{table}/{id}\",\n        name=f\"Record {id} from {table}\",\n        description=f\"Database record\",\n        mimeType=\"application/json\",\n        text=json.dumps(record)\n    )\n\n# Tools can reference resources\n@server.tool()\nasync def analyze_file(file_path: str) -> dict:\n    \"\"\"Analyze a file using the resource system\"\"\"\n    # Get file as resource\n    resource = await get_file_resource(file_path)\n    \n    # Analyze content\n    analysis = {\n        \"line_count\": len(resource.text.split('\\n')),\n        \"word_count\": len(resource.text.split()),\n        \"char_count\": len(resource.text)\n    }\n    \n    return analysis\n```\n\n**Resource Benefits:**\n- Standardized access to external data\n- Caching and optimization opportunities\n- Security and permission management\n- Content type handling"
  },
  {
    "topic": "Integration with Different LLMs",
    "type": "integration",
    "difficulty": "intermediate",
    "content": "MCP's vendor-neutral design allows integration with various LLM providers:\n\n**1. Claude Integration**\n```python\nimport anthropic\nfrom mcp.client import MCPClient\n\nclass ClaudeMCPIntegration:\n    def __init__(self, mcp_client: MCPClient):\n        self.claude = anthropic.Anthropic()\n        self.mcp_client = mcp_client\n    \n    async def chat_with_tools(self, message: str):\n        # Get available tools\n        tools = await self.mcp_client.list_tools()\n        \n        # Convert MCP tools to Claude format\n        claude_tools = self._convert_tools(tools)\n        \n        # Send to Claude\n        response = await self.claude.messages.create(\n            model=\"claude-3-sonnet-20240229\",\n            messages=[{\"role\": \"user\", \"content\": message}],\n            tools=claude_tools\n        )\n        \n        # Handle tool calls\n        if response.stop_reason == \"tool_use\":\n            for tool_call in response.content:\n                if tool_call.type == \"tool_use\":\n                    result = await self.mcp_client.call_tool(\n                        tool_call.name, \n                        tool_call.input\n                    )\n                    # Continue conversation with result\n        \n        return response\n```\n\n**2. Local Model Integration (Ollama)**\n```python\nimport requests\nfrom mcp.client import MCPClient\n\nclass OllamaMCPIntegration:\n    def __init__(self, mcp_client: MCPClient, model: str = \"llama2\"):\n        self.mcp_client = mcp_client\n        self.model = model\n        self.base_url = \"http://localhost:11434\"\n    \n    async def generate_with_tools(self, prompt: str):\n        # Get available tools\n        tools = await self.mcp_client.list_tools()\n        \n        # Add tools to system prompt\n        system_prompt = self._build_system_prompt(tools)\n        full_prompt = f\"{system_prompt}\\n\\nUser: {prompt}\\nAssistant:\"\n        \n        # Generate response\n        response = requests.post(\n            f\"{self.base_url}/api/generate\",\n            json={\n                \"model\": self.model,\n                \"prompt\": full_prompt,\n                \"stream\": False\n            }\n        )\n        \n        # Parse for tool calls and execute\n        result = response.json()[\"response\"]\n        tool_calls = self._extract_tool_calls(result)\n        \n        for call in tool_calls:\n            tool_result = await self.mcp_client.call_tool(\n                call[\"name\"], \n                call[\"parameters\"]\n            )\n            # Inject result back into conversation\n        \n        return result\n```\n\n**3. Gemini Integration**\n```python\nimport google.generativeai as genai\nfrom mcp.client import MCPClient\n\nclass GeminiMCPIntegration:\n    def __init__(self, mcp_client: MCPClient):\n        self.model = genai.GenerativeModel('gemini-pro')\n        self.mcp_client = mcp_client\n    \n    async def chat_with_tools(self, message: str):\n        # Get available tools\n        tools = await self.mcp_client.list_tools()\n        \n        # Configure Gemini with tools\n        gemini_tools = self._convert_to_gemini_format(tools)\n        \n        # Start chat with tools\n        chat = self.model.start_chat(tools=gemini_tools)\n        response = await chat.send_message_async(message)\n        \n        # Handle function calls\n        for part in response.parts:\n            if hasattr(part, 'function_call'):\n                fc = part.function_call\n                result = await self.mcp_client.call_tool(\n                    fc.name, \n                    dict(fc.args)\n                )\n                # Send result back to Gemini\n                response = await chat.send_message_async(\n                    genai.types.FunctionResponse(\n                        name=fc.name,\n                        response=result\n                    )\n                )\n        \n        return response.text\n```"
  },
  {
    "topic": "Best Practices and Performance",
    "type": "best_practices",
    "difficulty": "intermediate",
    "content": "Best practices for building robust MCP servers:\n\n**1. Error Handling**\n```python\nfrom mcp.types import MCPError\n\n@server.tool()\nasync def robust_tool(param: str) -> dict:\n    \"\"\"Tool with proper error handling\"\"\"\n    try:\n        # Validate input\n        if not param or len(param.strip()) == 0:\n            raise MCPError(\"INVALID_PARAMS\", \"Parameter cannot be empty\")\n        \n        # Process with timeout\n        result = await asyncio.wait_for(\n            process_data(param), \n            timeout=30.0\n        )\n        \n        return {\"status\": \"success\", \"data\": result}\n        \n    except asyncio.TimeoutError:\n        raise MCPError(\"TIMEOUT\", \"Operation timed out\")\n    except ValueError as e:\n        raise MCPError(\"INVALID_PARAMS\", str(e))\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        raise MCPError(\"INTERNAL_ERROR\", \"An internal error occurred\")\n```\n\n**2. Resource Management**\n```python\nclass ResourceManagedServer:\n    def __init__(self):\n        self.server = Server(\"resource-managed\")\n        self.connection_pool = None\n        self.cache = {}\n    \n    async def __aenter__(self):\n        # Initialize resources\n        self.connection_pool = await create_db_pool()\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        # Cleanup resources\n        if self.connection_pool:\n            await self.connection_pool.close()\n    \n    @server.tool()\n    async def cached_query(self, query: str) -> dict:\n        \"\"\"Query with caching\"\"\"\n        cache_key = hash(query)\n        \n        if cache_key in self.cache:\n            return self.cache[cache_key]\n        \n        async with self.connection_pool.acquire() as conn:\n            result = await conn.fetch(query)\n            self.cache[cache_key] = result\n            return result\n```\n\n**3. Performance Optimization**\n```python\n# Batch operations\n@server.tool()\nasync def batch_process(items: list[str]) -> list[dict]:\n    \"\"\"Process multiple items efficiently\"\"\"\n    # Process in batches to avoid overwhelming resources\n    batch_size = 10\n    results = []\n    \n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        batch_results = await asyncio.gather(\n            *[process_item(item) for item in batch],\n            return_exceptions=True\n        )\n        results.extend(batch_results)\n    \n    return results\n\n# Connection pooling\nfrom asyncio import Semaphore\nconnection_semaphore = Semaphore(5)  # Max 5 concurrent connections\n\n@server.tool()\nasync def limited_resource_tool(param: str) -> str:\n    \"\"\"Tool with connection limiting\"\"\"\n    async with connection_semaphore:\n        # Only 5 of these can run concurrently\n        return await expensive_operation(param)\n```\n\n**4. Testing Strategies**\n```python\nimport pytest\nfrom mcp.client import MCPClient\n\n@pytest.fixture\nasync def mcp_client():\n    # Start test server\n    client = MCPClient(transport=\"stdio\", command=[\"python\", \"test_server.py\"])\n    async with client:\n        yield client\n\n@pytest.mark.asyncio\nasync def test_tool_functionality(mcp_client):\n    \"\"\"Test tool behavior\"\"\"\n    result = await mcp_client.call_tool(\"test_tool\", {\"param\": \"test_value\"})\n    assert result[\"status\"] == \"success\"\n    assert \"data\" in result\n\n@pytest.mark.asyncio\nasync def test_error_handling(mcp_client):\n    \"\"\"Test error conditions\"\"\"\n    with pytest.raises(MCPError):\n        await mcp_client.call_tool(\"test_tool\", {\"invalid\": \"param\"})\n```\n\n**5. Monitoring and Logging**\n```python\nimport time\nfrom functools import wraps\n\ndef monitor_tool(func):\n    \"\"\"Decorator to monitor tool performance\"\"\"\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        start_time = time.time()\n        try:\n            result = await func(*args, **kwargs)\n            duration = time.time() - start_time\n            logger.info(f\"Tool {func.__name__} completed in {duration:.2f}s\")\n            return result\n        except Exception as e:\n            duration = time.time() - start_time\n            logger.error(f\"Tool {func.__name__} failed after {duration:.2f}s: {e}\")\n            raise\n    return wrapper\n\n@server.tool()\n@monitor_tool\nasync def monitored_tool(param: str) -> str:\n    \"\"\"Tool with monitoring\"\"\"\n    return await process_data(param)\n```"
  }
]
